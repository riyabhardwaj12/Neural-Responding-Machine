{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained Paragraph:- \n",
      "Two households, both alike in dignity\n",
      "(In fair Verona, where we lay our scene),\n",
      "From ancient grudge break to new mutiny,\n",
      "Where civil blood makes civil hands unclean.\n",
      "5From forth the fatal loins of these two foes\n",
      "A pair of star-crossed lovers take their life,\n",
      "Whose misadventured piteous overthrows\n",
      "Doth with their death bury their parents' strife.\n",
      "The fearful passage of their death-marked love\n",
      "10And the continuance of their parents' rage,\n",
      "Which, but their children’s end, naught could remove,\n",
      "Is now the two hours' traffic of our stage—\n",
      "The which, if you with patient ears attend,\n",
      "What here shall miss, our toil shall strive to mend.\n",
      "\n",
      "Our X_train 2-D Array:-\n",
      "[list([0, 122, 123, 34, 124, 125, 76, 126, 68, 127, 128, 129, 34, 130, 65, 131, 132, 133, 71, 34, 134, 135, 136, 137, 31, 138, 139, 34, 140, 141, 142, 143, 141, 144, 145, 26])\n",
      " list([0, 146, 147, 12, 148, 149, 19, 150, 151, 152, 27, 153, 19, 154, 155, 156, 157, 158, 34, 159, 160, 161, 162, 163, 164, 157, 165, 166, 157, 167, 168, 169, 26])\n",
      " list([0, 17, 170, 171, 19, 157, 172, 21, 173, 12, 174, 19, 157, 167, 168, 175, 34, 176, 34, 35, 157, 177, 178, 179, 180, 34, 181, 107, 182, 34, 183, 184, 12, 151, 185, 168, 186, 19, 132, 187, 17, 188, 34, 189, 59, 164, 190, 191, 192, 34, 193, 15, 194, 195, 34, 132, 196, 194, 197, 31, 198, 26])]\n",
      "\n",
      "Our Y_train 2-D Array:-\n",
      "[list([122, 123, 34, 124, 125, 76, 126, 68, 127, 128, 129, 34, 130, 65, 131, 132, 133, 71, 34, 134, 135, 136, 137, 31, 138, 139, 34, 140, 141, 142, 143, 141, 144, 145, 26, 8])\n",
      " list([146, 147, 12, 148, 149, 19, 150, 151, 152, 27, 153, 19, 154, 155, 156, 157, 158, 34, 159, 160, 161, 162, 163, 164, 157, 165, 166, 157, 167, 168, 169, 26, 8])\n",
      " list([17, 170, 171, 19, 157, 172, 21, 173, 12, 174, 19, 157, 167, 168, 175, 34, 176, 34, 35, 157, 177, 178, 179, 180, 34, 181, 107, 182, 34, 183, 184, 12, 151, 185, 168, 186, 19, 132, 187, 17, 188, 34, 189, 59, 164, 190, 191, 192, 34, 193, 15, 194, 195, 34, 132, 196, 194, 197, 31, 198, 26, 8])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "from utils import *\n",
    "import operator\n",
    "from datetime import *\n",
    "import sys\n",
    "\n",
    "#test=1\n",
    "f=open(\"Sentence.txt\",\"r\")\n",
    "x= f.read()\n",
    "print()\n",
    "print(\"Trained Paragraph:- \" )\n",
    "print(x)\n",
    "print()\n",
    "row_of_sentences=sent_tokenize(x);\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tok=tokenizer.tokenize(x)\n",
    "#tok.insert(0,\"SENTENCE_START\")\n",
    "#tok.append(\"SENTENCE_END\")\n",
    "\n",
    "for i in range (len(row_of_sentences)):\n",
    "    tok=word_tokenize(row_of_sentences[i])  #list type\n",
    "    tok.insert(0,\"sent_start\")\n",
    "    tok.append(\"sent_end\")\n",
    "    row_of_sentences[i]=tok;\n",
    "    #print(tok)\n",
    "    \n",
    "df=pd.read_csv(\"final.csv\",names=['Words'])  \n",
    "df=df['Words']\n",
    "#df.loc[0]='sent_start'\n",
    "voc=df.values.tolist()\n",
    "\n",
    "\n",
    "#vecx=[]\n",
    "#vecy=[]\n",
    "index_to_word=dict()\n",
    "word_to_index=dict()\n",
    "xtrainlist=[]\n",
    "ytrainlist=[]\n",
    "\n",
    "for i in row_of_sentences:\n",
    "    vecx=[]\n",
    "    for j in i[:-1]:\n",
    "        #print (j)\n",
    "        if(j in voc):\n",
    "            x=voc.index(j)\n",
    "            vecx.append(x)\n",
    "            index_to_word[x]=j\n",
    "            word_to_index[j]=x\n",
    "        else:\n",
    "            x=len(voc)\n",
    "            df.loc[len]=j\n",
    "            df.to_csv(\"final.csv\")\n",
    "            voc=df.values.tolist()\n",
    "            vecx.append(x)\n",
    "            index_to_word[x]=j\n",
    "            word_to_index[j]=x\n",
    "    xtrainlist.append(vecx)\n",
    "\n",
    "\n",
    "for i in range(len(row_of_sentences)):\n",
    "    vecy=[]\n",
    "    for j in range(1,len(xtrainlist[i])):\n",
    "        vecy.append(xtrainlist[i][j])\n",
    "    x=voc.index(\"sent_end\")\n",
    "    vecy.append(x)    \n",
    "    ytrainlist.append(vecy)\n",
    "    \n",
    "x=voc.index(\"sent_end\")\n",
    "index_to_word[x]=\"sent_end\"\n",
    "word_to_index[\"sent_end\"]=x \n",
    "\n",
    "\n",
    "X_train=np.asarray(xtrainlist)\n",
    "Y_train=np.asarray(ytrainlist)\n",
    "\n",
    "print(\"Our X_train 2-D Array:-\")\n",
    "print(X_train)\n",
    "print()\n",
    "\n",
    "print(\"Our Y_train 2-D Array:-\")\n",
    "print(Y_train)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.h=np.random.uniform(0,1,hidden_dim)\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    " \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        #with open('U.txt') as file:\n",
    "         #    dLdU= [[float(digit) for digit in line.split()] for line in file]\n",
    "        #with open('V.txt') as file:\n",
    "         #    dLdV= [[float(digit) for digit in line.split()] for line in file]\n",
    "        #with open('W.txt') as file:\n",
    "         #    dLdW= [[float(digit) for digit in line.split()] for line in file]\n",
    "\n",
    "        #dLdU=np.asarray(dLdU)\n",
    "        #dLdV=np.asarray(dLdV)\n",
    "        #dLdW=np.asarray(dLdW)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "\n",
    "        \n",
    "        delta_o = o\n",
    "        #print(\"heya\")\n",
    "        #print(delta_o)\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            #print(delta_o[t].shape)\n",
    "            #print(s[t].shape)\n",
    "            #print(s[t])\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        #a = np.asarray(dLdU)\n",
    "        #np.savetxt(\"U.txt\", a, delimiter=\" \")\n",
    "        #a = np.asarray(dLdV)\n",
    "        #np.savetxt(\"V.txt\", a, delimiter=\" \")\n",
    "        #a = np.asarray(dLdW)\n",
    "        #np.savetxt(\"W.txt\", a, delimiter=\" \")\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "\n",
    "\n",
    "\n",
    "    # Performs one step of SGD.\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    " \n",
    "\n",
    "    # Outer SGD Loop\n",
    "    # - model: The RNN model instance\n",
    "    # - X_train: The training data set\n",
    "    # - y_train: The training data labels\n",
    "    # - learning_rate: Initial learning rate for SGD\n",
    "    # - nepoch: Number of times to iterate through the complete dataset\n",
    "    # - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "    \n",
    "    def train_with_sgd(self, X_train, y_train, learning_rate=0.005, nepoch=200, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (\"%s: Loss after  epoch=%d: %f\" % (time, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5 \n",
    "                    print (\"Setting learning rate to %f\" % learning_rate)\n",
    "                    print()\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def generate_sentence(self,word):\n",
    "        x=1\n",
    "        # We start the sentence with the given start word\n",
    "        new_sentence = [voc.index(word)]\n",
    "        sent=word\n",
    "        while (not new_sentence[-1] == voc.index(\"sent_end\") and (not x>7)):\n",
    "            x+=1;\n",
    "            oo,ss=self.forward_propagation(new_sentence)\n",
    "            l=len(ss)\n",
    "            #print(\"seee\",l)\n",
    "            ct=np.random.uniform(0,0,l)\n",
    "            \n",
    "            for i in range(l):\n",
    "                for j in range(len(ss[i])):\n",
    "                    ct[i]+=self.h[i]*ss[i][j]\n",
    "\n",
    "            #print(\"cccc\")\n",
    "            #print(ct)\n",
    "            n_w=np.array(ct)\n",
    "            s_w=np.argmax(n_w)\n",
    "            #sent=sent.join(\" \")\n",
    "            if(not (voc[s_w]==\"sent_end\")):\n",
    "                if(not voc[s_w]==\"sent_start\"  ):\n",
    "                    sent=sent+\" \" +(voc[s_w])\n",
    "            new_sentence.append(s_w) \n",
    "            #next_word_probs = self.forward_propagation(new_sentence)\n",
    "            #sent_len+=1;\n",
    "\n",
    "            \n",
    "        # sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "        print(\"Generated text\")\n",
    "        print(sent)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_rnn=RNNNumpy(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training our model with X_tarin and Y_Train\n",
      "2018-11-23 12:32:19: Loss after  epoch=0: 5.309097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riya Bhardwaj\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-23 12:32:20: Loss after  epoch=5: 5.287195\n",
      "2018-11-23 12:32:20: Loss after  epoch=10: 5.264466\n",
      "2018-11-23 12:32:21: Loss after  epoch=15: 5.239795\n",
      "2018-11-23 12:32:22: Loss after  epoch=20: 5.211632\n",
      "2018-11-23 12:32:22: Loss after  epoch=25: 5.177349\n",
      "2018-11-23 12:32:23: Loss after  epoch=30: 5.130940\n",
      "2018-11-23 12:32:24: Loss after  epoch=35: 5.028362\n",
      "2018-11-23 12:32:24: Loss after  epoch=40: 4.666059\n",
      "2018-11-23 12:32:25: Loss after  epoch=45: 4.399147\n",
      "2018-11-23 12:32:25: Loss after  epoch=50: 4.306777\n",
      "2018-11-23 12:32:26: Loss after  epoch=55: 3.963999\n",
      "2018-11-23 12:32:26: Loss after  epoch=60: 3.844543\n",
      "2018-11-23 12:32:27: Loss after  epoch=65: 3.265305\n",
      "2018-11-23 12:32:28: Loss after  epoch=70: 3.159116\n",
      "2018-11-23 12:32:28: Loss after  epoch=75: 3.086420\n",
      "2018-11-23 12:32:29: Loss after  epoch=80: 2.619322\n",
      "2018-11-23 12:32:29: Loss after  epoch=85: 2.204422\n",
      "2018-11-23 12:32:30: Loss after  epoch=90: 1.413546\n",
      "2018-11-23 12:32:30: Loss after  epoch=95: 1.069032\n",
      "2018-11-23 12:32:31: Loss after  epoch=100: 0.823144\n",
      "2018-11-23 12:32:32: Loss after  epoch=105: 0.654721\n",
      "2018-11-23 12:32:32: Loss after  epoch=110: 0.535487\n",
      "2018-11-23 12:32:33: Loss after  epoch=115: 0.449278\n",
      "2018-11-23 12:32:33: Loss after  epoch=120: 0.385140\n",
      "2018-11-23 12:32:34: Loss after  epoch=125: 0.335954\n",
      "2018-11-23 12:32:34: Loss after  epoch=130: 0.297328\n",
      "2018-11-23 12:32:35: Loss after  epoch=135: 0.266485\n",
      "2018-11-23 12:32:36: Loss after  epoch=140: 0.241488\n",
      "2018-11-23 12:32:36: Loss after  epoch=145: 0.220884\n",
      "2018-11-23 12:32:37: Loss after  epoch=150: 0.203576\n",
      "2018-11-23 12:32:37: Loss after  epoch=155: 0.188754\n",
      "2018-11-23 12:32:38: Loss after  epoch=160: 0.175836\n",
      "2018-11-23 12:32:38: Loss after  epoch=165: 0.164413\n",
      "2018-11-23 12:32:39: Loss after  epoch=170: 0.154199\n",
      "2018-11-23 12:32:39: Loss after  epoch=175: 0.144997\n",
      "2018-11-23 12:32:40: Loss after  epoch=180: 0.136668\n",
      "2018-11-23 12:32:41: Loss after  epoch=185: 0.129112\n",
      "2018-11-23 12:32:41: Loss after  epoch=190: 0.122256\n",
      "2018-11-23 12:32:42: Loss after  epoch=195: 0.116036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Training our model with X_tarin and Y_Train\")\n",
    "our_rnn.train_with_sgd(X_train,Y_train)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : the\n",
      "Generated text\n",
      "the river All that that that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "our_rnn.generate_sentence(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : The\n",
      "Generated text\n",
      "The river All All glitters is is gold gold gold\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "our_rnn.generate_sentence(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : All\n",
      "Generated text\n",
      "All river glitters glitters glitters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "our_rnn.generate_sentence(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : Today\n",
      "Generated text\n",
      "Today river All that glitters glitters is is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "our_rnn.generate_sentence(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
