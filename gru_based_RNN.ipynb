{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trained Paragraph:- \n",
      "Two households, both alike in dignity\n",
      "(In fair Verona, where we lay our scene),\n",
      "From ancient grudge break to new mutiny,\n",
      "Where civil blood makes civil hands unclean.\n",
      "5From forth the fatal loins of these two foes\n",
      "A pair of star-crossed lovers take their life,\n",
      "Whose misadventured piteous overthrows\n",
      "Doth with their death bury their parents' strife.\n",
      "The fearful passage of their death-marked love\n",
      "10And the continuance of their parents' rage,\n",
      "Which, but their children’s end, naught could remove,\n",
      "Is now the two hours' traffic of our stage—\n",
      "The which, if you with patient ears attend,\n",
      "What here shall miss, our toil shall strive to mend.\n",
      "\n",
      "Our X_train 2-D Array:-\n",
      "[list([0, 122, 123, 34, 124, 125, 76, 126, 68, 127, 128, 129, 34, 130, 65, 131, 132, 133, 71, 34, 134, 135, 136, 137, 31, 138, 139, 34, 140, 141, 142, 143, 141, 144, 145, 26])\n",
      " list([0, 146, 147, 12, 148, 149, 19, 150, 151, 152, 27, 153, 19, 154, 155, 156, 157, 158, 34, 159, 160, 161, 162, 163, 164, 157, 165, 166, 157, 167, 168, 169, 26])\n",
      " list([0, 17, 170, 171, 19, 157, 172, 21, 173, 12, 174, 19, 157, 167, 168, 175, 34, 176, 34, 35, 157, 177, 178, 179, 180, 34, 181, 107, 182, 34, 183, 184, 12, 151, 185, 168, 186, 19, 132, 187, 17, 188, 34, 189, 59, 164, 190, 191, 192, 34, 193, 15, 194, 195, 34, 132, 196, 194, 197, 31, 198, 26])]\n",
      "\n",
      "Our Y_train 2-D Array:-\n",
      "[list([122, 123, 34, 124, 125, 76, 126, 68, 127, 128, 129, 34, 130, 65, 131, 132, 133, 71, 34, 134, 135, 136, 137, 31, 138, 139, 34, 140, 141, 142, 143, 141, 144, 145, 26, 8])\n",
      " list([146, 147, 12, 148, 149, 19, 150, 151, 152, 27, 153, 19, 154, 155, 156, 157, 158, 34, 159, 160, 161, 162, 163, 164, 157, 165, 166, 157, 167, 168, 169, 26, 8])\n",
      " list([17, 170, 171, 19, 157, 172, 21, 173, 12, 174, 19, 157, 167, 168, 175, 34, 176, 34, 35, 157, 177, 178, 179, 180, 34, 181, 107, 182, 34, 183, 184, 12, 151, 185, 168, 186, 19, 132, 187, 17, 188, 34, 189, 59, 164, 190, 191, 192, 34, 193, 15, 194, 195, 34, 132, 196, 194, 197, 31, 198, 26, 8])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import operator\n",
    "#from utils import load_data, load_model_parameters_theano, generate_sentences\n",
    "from gru_theano import *\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "import operator\n",
    "from datetime import *\n",
    "import sys\n",
    "\n",
    "#test=1\n",
    "f=open(\"Sentence.txt\",\"r\")\n",
    "x= f.read()\n",
    "print()\n",
    "print(\"Trained Paragraph:- \" )\n",
    "print(x)\n",
    "print()\n",
    "row_of_sentences=sent_tokenize(x);\n",
    "\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tok=tokenizer.tokenize(x)\n",
    "#tok.insert(0,\"SENTENCE_START\")\n",
    "#tok.append(\"SENTENCE_END\")\n",
    "\n",
    "for i in range (len(row_of_sentences)):\n",
    "    tok=word_tokenize(row_of_sentences[i])  #list type\n",
    "    tok.insert(0,\"sent_start\")\n",
    "    tok.append(\"sent_end\")\n",
    "    row_of_sentences[i]=tok;\n",
    "    #print(tok)\n",
    "    \n",
    "df=pd.read_csv(\"final.csv\",names=['Words'])  \n",
    "df=df['Words']\n",
    "#df.loc[0]='sent_start'\n",
    "voc=df.values.tolist()\n",
    "\n",
    "\n",
    "#vecx=[]\n",
    "#vecy=[]\n",
    "index_to_word=dict()\n",
    "word_to_index=dict()\n",
    "xtrainlist=[]\n",
    "ytrainlist=[]\n",
    "\n",
    "for i in row_of_sentences:\n",
    "    vecx=[]\n",
    "    for j in i[:-1]:\n",
    "        #print (j)\n",
    "        if(j in voc):\n",
    "            x=voc.index(j)\n",
    "            vecx.append(x)\n",
    "            index_to_word[x]=j\n",
    "            word_to_index[j]=x\n",
    "        else:\n",
    "            x=len(voc)\n",
    "            df.loc[len]=j\n",
    "            df.to_csv(\"final.csv\")\n",
    "            voc=df.values.tolist()\n",
    "            vecx.append(x)\n",
    "            index_to_word[x]=j\n",
    "            word_to_index[j]=x\n",
    "    xtrainlist.append(vecx)\n",
    "\n",
    "\n",
    "for i in range(len(row_of_sentences)):\n",
    "    vecy=[]\n",
    "    for j in range(1,len(xtrainlist[i])):\n",
    "        vecy.append(xtrainlist[i][j])\n",
    "    x=voc.index(\"sent_end\")\n",
    "    vecy.append(x)    \n",
    "    ytrainlist.append(vecy)\n",
    "    \n",
    "x=voc.index(\"sent_end\")\n",
    "index_to_word[x]=\"sent_end\"\n",
    "word_to_index[\"sent_end\"]=x \n",
    "\n",
    "\n",
    "X_train=np.asarray(xtrainlist)\n",
    "Y_train=np.asarray(ytrainlist)\n",
    "\n",
    "print(\"Our X_train 2-D Array:-\")\n",
    "print(X_train)\n",
    "print()\n",
    "\n",
    "print(\"Our Y_train 2-D Array:-\")\n",
    "print(Y_train)\n",
    "print()\n",
    "\n",
    "VOCABULARY_SIZE = len(voc)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riya Bhardwaj\\gru_theano.py:73: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
      "  o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n",
      "C:\\Users\\Riya Bhardwaj\\gru_theano.py:118: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  [x, y, learning_rate, theano.Param(decay, default=0.9)],\n"
     ]
    }
   ],
   "source": [
    "model = GRUTheano(VOCABULARY_SIZE, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: ~14344.945908 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], Y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print (\"SGD Step time: ~%f milliseconds\" % ((t2 - t1) * 1000.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gru_theano.GRUTheano at 0x2402ee87ef0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_sgd(model, X_train, Y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utilsss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : The\n",
      "Generated text\n",
      "The of A death-marked love A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "generate_sentence(model,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : All\n",
      "Generated text\n",
      "All where blood new makes naught\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "generate_sentence(model,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word to start sentence from : Today\n",
      "Generated text\n",
      "Today Whether death-marked these two two\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=input(\"Enter word to start sentence from : \")\n",
    "generate_sentence(model,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
